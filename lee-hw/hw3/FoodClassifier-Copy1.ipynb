{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"E:\\\\PycharmProjects\\\\venv\\\\food-11\\\\training\\\\0_0.jpg\")\n",
    "data = np.array(img)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    # label 是一個 boolean variable，代表需不需要回傳 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        x[i, :, :] = cv2.resize(img,(128, 128))\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "workspace_dir = 'E:\\\\PycharmProjects\\\\venv\\\\food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9866, 128, 128, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training 時做 data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), #隨機將圖片水平翻轉\n",
    "    transforms.RandomRotation(15), #隨機旋轉圖片\n",
    "    transforms.ToTensor(), #將圖片轉成 Tensor，並把數值normalize到[0,1](data normalization)\n",
    "])\n",
    "#testing 時不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = ImgDataset(train_x, train_y, train_transform)\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output_shape = (image_shape-filter_shape+2*padding)/stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "    \n",
    "        self.cnn = nn.Sequential(\n",
    "            # 128*128*3\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), # 64*128*128 \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),   # 64*64*64\n",
    "        \n",
    "            nn.Conv2d(64, 128, 3, 1, 1), #128*64*64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #128*32*32\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1), #256*32*32\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), #256*16*16\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1), #512*16*16\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #512*8*8\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1),#512*8*8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),  #512*4*4\n",
    "        \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 11),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 993.69 sec(s) Train Acc: 0.238090 Loss: 0.017549 | Val Acc: 0.314869 loss: 0.015425\n",
      "[002/030] 834.68 sec(s) Train Acc: 0.348368 Loss: 0.014575 | Val Acc: 0.363265 loss: 0.014492\n",
      "[003/030] 847.74 sec(s) Train Acc: 0.403406 Loss: 0.013500 | Val Acc: 0.391254 loss: 0.013438\n",
      "[004/030] 676.45 sec(s) Train Acc: 0.446077 Loss: 0.012590 | Val Acc: 0.444606 loss: 0.012606\n",
      "[005/030] 647.74 sec(s) Train Acc: 0.474356 Loss: 0.011899 | Val Acc: 0.460058 loss: 0.011968\n",
      "[006/030] 655.08 sec(s) Train Acc: 0.507298 Loss: 0.011139 | Val Acc: 0.481341 loss: 0.012068\n",
      "[007/030] 751.66 sec(s) Train Acc: 0.530712 Loss: 0.010665 | Val Acc: 0.515452 loss: 0.011290\n",
      "[008/030] 769.25 sec(s) Train Acc: 0.548956 Loss: 0.010302 | Val Acc: 0.559767 loss: 0.010078\n",
      "[009/030] 679.19 sec(s) Train Acc: 0.594364 Loss: 0.009298 | Val Acc: 0.565889 loss: 0.009827\n",
      "[010/030] 665.98 sec(s) Train Acc: 0.600345 Loss: 0.008944 | Val Acc: 0.567055 loss: 0.010113\n",
      "[011/030] 668.03 sec(s) Train Acc: 0.630651 Loss: 0.008468 | Val Acc: 0.600000 loss: 0.009300\n",
      "[012/030] 667.07 sec(s) Train Acc: 0.641901 Loss: 0.008106 | Val Acc: 0.617493 loss: 0.008894\n",
      "[013/030] 709.18 sec(s) Train Acc: 0.674336 Loss: 0.007468 | Val Acc: 0.613703 loss: 0.009311\n",
      "[014/030] 760.27 sec(s) Train Acc: 0.678796 Loss: 0.007369 | Val Acc: 0.621866 loss: 0.008979\n",
      "[015/030] 760.75 sec(s) Train Acc: 0.697040 Loss: 0.006837 | Val Acc: 0.608455 loss: 0.009486\n",
      "[016/030] 760.34 sec(s) Train Acc: 0.693493 Loss: 0.007002 | Val Acc: 0.643149 loss: 0.008658\n",
      "[017/030] 761.12 sec(s) Train Acc: 0.724103 Loss: 0.006200 | Val Acc: 0.643440 loss: 0.008684\n",
      "[018/030] 765.20 sec(s) Train Acc: 0.742753 Loss: 0.005865 | Val Acc: 0.641399 loss: 0.008540\n",
      "[019/030] 759.97 sec(s) Train Acc: 0.738293 Loss: 0.005935 | Val Acc: 0.655977 loss: 0.008534\n",
      "[020/030] 761.00 sec(s) Train Acc: 0.754612 Loss: 0.005459 | Val Acc: 0.682216 loss: 0.008130\n",
      "[021/030] 765.42 sec(s) Train Acc: 0.770728 Loss: 0.005212 | Val Acc: 0.654810 loss: 0.008801\n",
      "[022/030] 730.88 sec(s) Train Acc: 0.780965 Loss: 0.004923 | Val Acc: 0.661516 loss: 0.008351\n",
      "[023/030] 669.25 sec(s) Train Acc: 0.780053 Loss: 0.004816 | Val Acc: 0.688921 loss: 0.008228\n",
      "[024/030] 670.26 sec(s) Train Acc: 0.814109 Loss: 0.004197 | Val Acc: 0.689796 loss: 0.008511\n",
      "[025/030] 670.06 sec(s) Train Acc: 0.816542 Loss: 0.004100 | Val Acc: 0.663848 loss: 0.009317\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(data[0])\n",
    "        batch_loss = loss(train_pred, data[1])\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += np.sum(np.argmax(train_pred.data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "    model.eval\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0])\n",
    "            batch_loss = loss(val_pred, data[1])\n",
    "            \n",
    "            val_acc += np.sum(np.argmax(val_pred.data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "            \n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
